<!DOCTYPE html>
<html lang="en">
<title>Artificial Intelligence</title>
<link rel="shortcut icon" href="ab.png" >
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128390657-1"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-128390657-1', {
  'page_path': location.pathname,
  'link_attribution': true,
});

gtag('event', 'page_view', { 'send_to': 'UA-128390657-1'});

(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "ca-pub-6187072965455073",
enable_page_level_ads: true
});

window.addEventListener('load', function()
{
   if(window.ga && ga.create) console.log('Google Analytics is loaded');
   else console.log('Google Analytics is not loaded');    

   if(window.google_tag_manager) console.log('Google Tag Manager is loaded');
   else console.log('Google Tag Manager is not loaded');
}, false);

window.texme = { style: 'plain' }
</script>
<script src="https://cdn.jsdelivr.net/npm/texme@0.5.0"></script>
<style>
a {
  text-decoration: inherit;
}
</style>
<img src="https://ga-beacon.appspot.com/UA-128390657-2/gh-pages/cs188?pixel" />
<textarea>

# CS 188: Artificial Intelligence

$$
\newcommand{\t}[1]{\text{#1}}
\newcommand{\op}[1]{\operatorname{#1}}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\brc}[1]{\lbrace #1 \rbrace}
\newcommand{\brk}[1]{[ #1 ]}
\newcommand{\P}[1]{\op{P}\brk{#1}}
\newcommand{\apx}[1]{\stackrel{\sim}{#1}} % \overset{\sim}{#1}
$$

## Graph Search for shortest/optimal paths

search problem - start, goal-test, actions, and transition (next possible state/successor)
 - state space can be tree (nodes/vertices are states, branches/edges are transitions, doesn't use/uses closed set to avoid revisiting, node in tree represents whole path in graph)

work on unweighted graphs (w/o edge costs)
- depth-first
  - uses a stack fringe to store its path, goes in one direction as far as possible before trying another
  - nonoptimal, graph search is complete for finite search spaces
- breadth-first
  - uses a queue fringe, checks children layer by later
  - complete, optimal for shortest number of edges in a path
- iterative deepening reruns depth-first at increasing levels

work on weighted graphs (uses priority queues)
- uniform-cost search - complete, optimal (minimizes backward distance (total/running cost))
- greedy search
  - nonoptimal, complete for finite graphs, minimizes forward distance (estimated by heuristic, makes the search *informed*)

A* - minimizes sum of forward and backward distance
 - A* tree search is optimal if the heuristic, $h$, is *admissible*, or $0 \le h(X) \le h^*(X)$
 - A* graph search is optimal if the heuristic, $h$, is *consistent*, or $h(X) - h(Y) \le d(X, Y)$
 - consistency implies admissability (lower bounds, optimistic)

## CSP for constrained variable-assignment problems

 - states are variables and domains
 - transition is the assignment of a variable to 1 value
 - goal is where every variable is assigned that satisfies all constraints
 - backtrack when a contraint is violated by a variable that has other values in its domain other than the one it has assigned

```pseudocode
def backtracking_search(csp, assignment) => map|failure
  if csp.satisfies(assignment)
    return assignment
  end
  var <- csp.select_unassigned_var(csp, assignment)
  for value <- csp.ordered_domain_vals(csp, var, assignment)
    if csp.consistent_with(value, assignments)
      assignment[var] <- val
      result <- csp.backtracking_search(assignment)
      if result is.not failure
        return result
      end
      remove assignment[var]
    end
  end
  return failure
end
```

### Improvements

filtering
 - an arc $X \to Y$ is consistent when all values for $X$ work with some value for $Y$
 - enforcing arc consistency involves crossing off values in $X$ that violate a (binary) constraint with all possible values of $Y$
 - forward checking is enforcing consistency of arcs pointing to each new assignment
 - forward checking: assign $X_i$ to a value, add all arcs that point to $X_i$, enforce arc consistency
 - AC3 (ensures that all arcs are consistent):
   - after assigning value to $X$, add all arcs to queue
   - for each item in queue, enforce the arc, and add any arcs that points to variables whose domain has changed to the queue
 - running time of AC3: $d$ is domain, $n$ variables, $T(n, d) = O(n^2d^3)$
   - $O(d^2)$ values checked between 2 domains in enforcing an arc
   - eliminating any value out of all domains $O(nd)$ triggers potentially $O(n)$ other arcs)

ordering
 - MRV (choses variable with smallest domain)
 - LCV (chose value rules out the least values)

solving tree-structured CSPs
 - choose minimum spanning tree directed out from a root, run topological sort, perform arc consistency in reverse linear order, assign in forward order
 - $O(nd^2)$ time to enforce arc consistency

cutset

min-conflicts

## Game Trees for adversarial problems

minimax is bound to yield a move that gives you a move that guarentees a value at least as good against any opponent, regardless of optimality, by assuming optimality

alpha-beta pruning
 - alpha: MAX's best option on path to root, applying downwards
 - beta: MIN's best option on path to root, applying downwards
 - if a MAX node comes across a value greater than beta (MIN will allow) and vv, prune rest of branches; otherwise it updates alpha or beta, respectively, for the nodes below it

expectiminimax - introduces chance nodes, which pass up the expected value of their children

non-zero-sum
 - have a value for each player, as they do not sum to zero
 - May not be adversarial anymore, is possible that two players can be cooperative
 - Pruning becomes more complicated (usually you can’t prune unless there is some bound on the utilities).

## MDP

Setting
 - $T(s, a, s') = \P{s' | s, a}$
 - $R(s, a, s')$
 - $U([s_0,s_1,s_2,...]) = \sum_{t \ge 0} \gamma^t R(s_t, a_t, s_{t+1}) \le R_{\max} \sum_{t \ge 0} \gamma^t = \frac{1}{1 - \gamma} R_{\max}$

Bellman equations
 - $Q(s, a) = \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma V(s')]$
 - $V(s) = \max_a Q(s, a)$

Value Iteration
 - initialize $V_0(s) = 0$, update $V_{k+1}(s) \gets \max_a Q_k(s, a)$, $V_k(s) \to V^*(s)$
 - Has per-iteration complexity of $O(\abs{S}^2\abs{A})$ since we consider every $(s,a,s’)$ combo

policies
 - $V^{\pi}(s) = \sum_{s'} T(s, \pi(s), s')[R(s, \pi(s), s') + \gamma V^{\pi}(s')]$
 - optimal policy $\pi^*(s) = \arg \max_a Q^*(s, a)$, $V^{\pi^*} = V^*$

Policy Iteration
 - policy evaluation: find $V^{\pi_i}$, in iterations that take $O(\abs{S}^2)$
 - update policy $\pi_{i+1}$ by maximizing policy Q values for every state, which takes $O(\abs{S}^2\abs{A})$
 - stop iterating when policy converges

## Reinforcement Learning

model-based - estimate T and R, solve like an MDP; offline

model-free - online
 - Temporal-Difference learning
   - passive (fixed policy)
   - for sample ($s$, $s'$, $r$), $V^\pi(s) \leftarrow \alpha (r + \gamma V^\pi(s')) + (1 - \alpha) V^\pi(s)$
   - every state is an exponential moving average of expected reward
 - Q-learning
   - active
   - for sample ($s$, $a$, $s'$, $r$), $Q(s, a) \leftarrow \alpha (r + \gamma V(s')) + (1 - \alpha) Q(s, a)$
   - decrease $\alpha$ over time
 - approximate Q-learning
   - $Q(s, a) = w \cdot f(s, a)$
   - $w \leftarrow w + \alpha (r + \gamma V(s') - Q(s, a)) f(s, a)$

## Bayes Nets

chain rule: $X \perp Y | Z \Leftrightarrow \P{X | Y, Z} = \P{X | Z}, \P{X, Y | Z} = \P{X | Z}\P{Y | Z}$

an arrow does not guarantee dependence, the values in the CPT could encode independence

[D-Separation](http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/)
 - active triples: causal chain, common cause, observed common effect
 - inactive triples: observed causal chain, observed common cause, common effect
 - if all triples in a path connecting the two nodes are active, the path is active
 - if no path is active, they are d-separated which makes them conditionally independent
 - otherwise, they are d-connected

inference by elimination
 - to eliminate a variable, create a factor with of its children given its parents to replace its children
 - to save space, try to eliminate variables with few incoming and outgoing connections 

### Sampling

faster way to approximate a query than inference
 - prior sampling
   - sample from joint, discard if full sample does not match evidence, infer by counting/tallying
 - rejection sampling
   - generate from top down, restart if partial doesn't match evidence
   - if the evidence is unlikely, we may reject a lot of samples
 - likelihood weighting
   - fix evidence variables, weight by $\P{\t{evidence variables}}$
 - Gibbs sampling
   - Fix the evidence variables
   - Initialize the non evidence variables randomly
   - Repeadedly reassign a non-evidence variable weighted by its probability given everything else
   - Wait a few reps for the variables to recover from the random initialization, then we count up the reassignments that match our query and then divide by the total reassignments.

### Decision Networks

 - introduce utility node with state and action parents
 - $\op{MEU}(e) = \max_a \sum_s \P{s | e} U(s, a)$
 - $\op{MEU}(e, E') = \sum_{e'} \P{e' | e} \op{MEU}(e, e')$
 - $\op{VPI}(E' | e) = \op{MEU}(e, E') - \op{MEU}(e)$
 - nonnegativity: $\op{VPI}(X) \ge 0$
 - order-independence: $\op{VPI}(X, Y) = \op{VPI}(X | Y) + \op{VPI}(Y) = \op{VPI}(X) + \op{VPI}(Y | X)$
 - if $E \perp U$, $\op{VPI}(E) = 0$

## Markov Models

 - linear Bayes net: first order sequence
 - mini-forward algorithm - calculate next state based on previous
 - HMM - partially observable states; underlying Markov model, but we only see observations
 - time elapse (dynamics update) - $X_{1:t-1} \to X_t | e_{1:t-1}$
 - observe update - $X_{1:t} \to e_t | e_{1:t-1}$

particle filtering
 - initial distribution $\P{X_0}$
 - for each $t$ (until convergence)
   - time update: sample $X_t$ from $\P{X_t | X_{t-1}}$ for each particle
   - obervation update: update distribution of $\P{X_t}$ using particle $X_t$ weighted by $\P{E_t | X_t}$
   - redraw particles all of equal weight

## MLE

 - Let $C(y) = \sum_i \mathbb{1}\brc{x_i = y}, C = \sum_i 1$
 - $\P{y} \approx \frac{C(y)}{C}$, $\P{a | y} \approx \frac{C(a, y)}{C(y)}$

$\theta \approx \arg \max_\theta \mathcal{L}(\theta)$, where $\mathcal{L}(\theta) = \prod_i \P{x_i | \theta}$
 - can be found at $D_\theta \log \mathcal{L}(\theta) = 0$ to avoid product rule

 - for Bernoulli r.v., $\P{x_i} \apx{\propto} C(x_i)$
 - laplace smoothing: $\P{x_i} \apx{\propto} C(x_i) + k$

MLE for discrete distributions 
 - Make a table of MLEs of probabilities for each assignment
 - Then use Bayes Theorem to assign a label by calculating the posteriors (probability of class given data) from the priors (probability of class) and the MLEs (probability of data given class)
 - Basically, we don’t know what we don’t know. Previously calculating MLE, we assigned 0 probability to something we haven’t seen.
 - So now, you’re assigning low probability to things you hadn’t seen. 

Laplace smoothing
 - adds “fake data” into our training set. The more “fake data” we add, the less the original sample distribution will matter.
 - If a feature never occurs a certain way during training, then our classifier will assume that the probability of it occurring is always 0, which is extremely drastic
 - more robust against unobserved values
 - Laplace Smoothing takes a CPT and a parameter $k$. Pretend to have seen $k$ more samples for each outcome.
 - $\P{a | y} = \frac{C(a, y)+k}{C(y) + k\abs{A}}$

## Naive Bayes

 - “Naive” because we assume features are conditionally independent given label
   - Canonical Example: “Bag of Words”
   -  $\P{y | x} \propto \arg \max_y \P{y} \prod_i \P{x_i | y}$
 - At training time, we build conditional probability tables based on our sample data.
 - At runtime, we observe features and classify using those conditional probability tables.
 - the label is $\arg \max_y \P{Y = y | \t{observed features}} \propto \arg \max_y \P{Y = y, \t{observed features}}$

</textarea>