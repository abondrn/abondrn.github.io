<!DOCTYPE html>
<html lang="en">
<title>CS61C</title>
<link rel="shortcut icon" href="ab.png" >
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128390657-1"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-128390657-1', {
  'page_path': location.pathname,
  'link_attribution': true,
});

gtag('event', 'page_view', { 'send_to': 'UA-128390657-1'});

(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "ca-pub-6187072965455073",
enable_page_level_ads: true
});

window.addEventListener('load', function()
{
   if(window.ga && ga.create) console.log('Google Analytics is loaded');
   else console.log('Google Analytics is not loaded');    

   if(window.google_tag_manager) console.log('Google Tag Manager is loaded');
   else console.log('Google Tag Manager is not loaded');
}, false);

window.texme = { style: 'plain' }
</script>
<script src="https://cdn.jsdelivr.net/npm/texme@0.5.0"></script>
<style>
a {
  text-decoration: inherit;
}
</style>
<img src="https://ga-beacon.appspot.com/UA-128390657-2/gh-pages/cs61c?pixel" />
<textarea>

# CS 61C: Computer Architecture

$$\newcommand{\w}[1]{\text{#1}}$$

## Misc.

signed = $0..2^{n}-1$, signed = $-2^{n-1}..2^{n-1}-1$

`-x = ~x + 1`

important powers of 2: `5:32 6:64 7:128 8:256 10:1024 ...`

arithmetic right shift always rounds towards negative infinity, logical right shift zero-fills

sign extension is padding everything to the left with the sign bit

int is 4 bytes

Memory:
 - Stack: holds procedure-local variables during a call, grows down in frames
 - Heap: dynamically allocated in main memory, grows up
 - Static: declared at the program level, global variables (incl. arrays) and (read only) string literals
 - Code: stores instructions and compile-time constants, read only, lowest memory addresses, loaded when program starts

assigning to a string or array constant is equivalent to assigning to the that static data

always check if a given pointer is null, or if the result of malloc was null

## RISC-V

RISC-V is little-endian (least-signifigant byte is first from left to right, `&b+1` moves to a more significant byte)

Assembly directive
`.data` - static data below
`.text` - code below
`.word w_1 ... w_n` - store n 32 bit quatites on sucessive memory words

 - Compiler: takes a high level language and converts to assembly
 - Assembler: takes assembly and outputs binary files - expand pseudo-instructions, generate symbol table, generate relocation table
 - Linker: takes multiple binary files and resolves absolute address references to output machine code
 - Loader: allocates memory for execution

## Instructions

`sp,gp,s0-1`: preserved across function call, caller can rely on values being unchanged

`a0-7,ra,t0-`: not preserved across function calls

assignment pseudo instructions

```assembly
mv rd, rs # addi rd, rs, 0; rd = rs
li rd, imm  # addi rd, zero, imm; rd = imm
transfer data between memory and registers
lw r1, o(r2) # load word; o is immediate offset is bytes, r2 is a base (pointer) register (holds an address); r1 = &r2[c]
sw r1, o(r2) # store word; r2[c] = r1
```

`lh` load halfword (sign-extends), `lhu` (unsigned)

`lb` and `sb` works the same for bytes, which are copied into registers by sign-extension

### Control

conditionals
 - `bne` (branch if not equal)
 - `blt` (branch on less than)
 - `bltu` (unsigned branch on less than)
 - PC-relative addressing: goto imm <-> PC += 4*(cond*imm + 1)

```assembly
beq r1, r2, label # branch if equal; if (r1 == r2) goto labe;
j label # jump (unconditional branch); goto label

j label: PC += offset (offset = sign-extended imm * 2)
jal: ra = PC + 4, j label
j label <-> jal zero, label
jalr: rd = PC + 4, PC = rs + offset
```
 - call function on any 32-bit absolute address: `lui r, <hi20bits>`; `jalr ra, r, <lo12bits>`
 - jump PC-relative with 32-bit offset: `auipc r, <hi20bits>`; `jalr zero, r, <lo12bits>`
 - `la r, <32bits>` $\Leftrightarrow$ `lui r, <hi20buts>`; `ori r, r, <lo12bits>`
 - `la r, label` gets address of label in `r`; `lui r, label`; `addi r, label`
 - branch: `PC += cond ? 2 * immediate : 4` $\Rightarrow$ immediate must be a multiple of 4
 - `ret` $\Leftrightarrow$ `jr ra` $\Leftrightarrow$ `jalr zero, ra, lbl`

 - multiplication: `mul` and `mulh`
 - division: `div` and `rem`

Instruction Format
 - imm are sign-extended to 32-bits for arithmetic operation
 - shift-by-immediate instructions use lower 5 bit
 - since J can address more memory than B, you could branch to a jump to have the same effect
 - to set a register to any 32-bit number, use LUI to set upper 20 and ADD to set lower 1

### Conventions

when calling function
1. store arguments in `a0-7 `as parameters to be passed
2. transfer control
```assembly
jal [ra,] fn-label # jump and link
lc ra PC+1 (save ra, link)
j fn-label
```
3. save values needed after call in stack
```assembly
push
addi sp, sp, -8
sw s1, 0(sp)
sw s2, 4(sp)
```
later...
```assembly
pop
sw s1, 0(sp)
sw s2, 4(sp)
addi sp, sp, 8
```
must do this for ra if doing nested call
when returning from function
1. store return value
put results in `a0-7`
2. restore any registers used
3. return control to point of origin
`ret # jump register; jr ra`
`label: auipc rd, 0 # puts address`

How to write a function call:
 - Before call
   - save contents of temporary and argument register
 - Prologue
   - save contents of `s0-11` registers to be used on the stack (except for temporaries) (first decrement `sp`)
   - this includes `ra` if there is a nested call to another function
 - Epilogue
   - restore contents of registers from stack (increment `sp` afterwards)
 - After call
   - use values `a0-1` accordingly

```assembly
# a0 -> A[c]
mv t0, a0 # int* A = a
li t1, 0 # int i=0
li t2, c # to be used in comparison
loop: # for (int i=0; i<c; i++)
	lw t3, 0(t0) # x3 = A[i]
	# do work her
	addi t0, t0, 4 # A++
	addi t1, t1, 1 # i++
	blt t1, t2, loop # if (i < c) goto loop
	s of label into r
```

## SDS

to find an expression for an FSM, first convert to a truth table and then simplify the sum-of-products expression
registers
 - value only updates at a clock rise/rising edge (0-1)
 - the input to a register needs to be stable **setup time** before and **hold time** after a clock cycle starts in order to be saved in the register and then successfully copied to the output during that time
 - **clk-q-delay** is the time it takes for a register's input to show up on the Q terminal (i.e. the output of the register) after a clock cycle. You can think of this as the logic delay for a register to output its result (just like the propagation delay for an AND gate to output its result)
 - critical path - longest delay from a memory output to a memory input across the circuit
For any circuit:
 - $t_{hold} \le t_{shortest path}$
 - $t_{longest path} + t_{setup} \le t_{clkcycle}$
 - $t_{clk-to-q} \w{(source)} + t_{logic} + t_{setup} \w{(dest)} \le t_{clkcycle}$

## Single-Cycle Datapath

CPU
 - PCSel - 0 if new PC is old PC+4, else 1 if jump/branch (was computed by ALU)
 - IMEM - fetch insturction
 - Imm. Gen. - ImmSel (reads opcode to determine format)
 - RegFile - RegWEn (does the instruction write to a register?)
 - BranchComp - BrEq, BrUn, BrLt
 - ALU - ASel, BSel, ALUSel (gets type of operation done, regardless of inputs)
 - DMEM - MemRW (does the instruction read from memory; only true for stores)
 - WBSel (determine what gets stores into `rd`; 2 is PC+4, 1 is result of ALU, 0 is memory)

## Pipeline/Hazards

by pipelining between stages, allows us to process multiple instruction at the same time
registers change amount of time between state elements, allowing the clock cycle to be faster
stages - insturction fetch, decode/register read, execute, memory, write back

$T_{c,pipeline} >= \frac{T_{c,single-cycle}}{\w{stages}}$
 - latency - time for one instruction to finish
   - latency increases for for pipeline version
   - single-cycle - add up delay for all stages
   - pipelined - stages * clock period
 - throughput - number of instruction processed per second
   - throughput increases for pipeline version
   - stages / latency

hazards (insturction conflicts)
 - structural (compete for single physical resource, i.e. memory or regfile)
 - data (dependency on previous instruction)
   - solution - stalling, forwarding (use results when computed), load delay slot, re-ordering instructions
 - control (dependency on previous instruction)
   - solution - kill next 2 (dead cycles=nop) for every branch taken; branch prediction, flush if wrong

## Caches

organization types
 - n-way associative - a block can go in any one of n places; this requires n comparisons and gives us (blocks / associativity) sets
 - fully associative (sets = blocks) - a block can go anywhere, no index
 - direct mapped (sets = 1) - a block can only go to one slot

how is address T:I:O mapped into a entry/slot?

 - index:  which set inside cache,          bits = log(sets)
 - offset: where inside block our data is,  bits = log(block bytes)
 - tag:    which block inside set,          bits = address bits - index bits - offset bits

if 2 addresses differ by a multiple of $2^{\w{offset} + \w{index bits}}$ they map into the same set

write policy
 - write hit policy
   - write back - write to cache, write to memory when evicted (dirty bit)
   - write through - write to both cache and memory
 - write miss policy
   - write allocate - put block inside cache and perform write hit policy vs just write
 - valid bit - cold
 - LRU bits - log(blocks)

miss types
 - compulsory - the data had not been used yet so it couldn't have been put into the cache
 - conflict - two blocks are mapped to the same index (fully associative cache has no conflict misses)
 - capacity - data was evicted before we could access

Average Memory Access Time = hit time + miss rate * miss penalty
 - multilevel caches
 - AMAT of next cache level becomes prevous miss penalty
 - global (divided by total access) vs local (divided by access at that level) miss rate

hit rate

## Floating Point

32 bit: sign(1) exponent(8) significand(23

$$\begin{array}
\w{object}  & \w{significand} & \w{exponent} \\
0           & 0               & 0            \\
\w{denorm}  & \w{nonzero}     & 0            \\
\w{inf}     & 0               & 255          \\
\w{NaN}     & \w{nonzero}     & 255          \\
\end{array}$$

 - normalized: $(-1)^\w{sign} \times 1.\w{significand} \times 2^\w{exponent + BIAS}$
 - denormalized: $(-1)^\w{sign} \times 0.\w{significand} \times 2^\w{exponent + BIAS + 1}$, can represent smaller numbers
 - $\w{BIAS} = -127$
 - zeroes: denorm, positive, and negative
 - largesty finite positive value - $2^{254 + \w{BIAS}}(2 - 2^{-23})$
 - smallest - $2^{-23} 2^{-126}$
 - smallest normalized - $2^{-126}$

## Cache Coherency

MOESI
 - Modified - Up-to-date data, changed (dirty), memory out-of-date, no other cache has copy
   - Only cache that supplies data on read instead of going to memory
 - Owned - Up-to-date data, other caches may have a copy (they must be in shared state)
 - Exclusive - Up-to-date data, no other cache has a copy, memory up-to-date
   - Supplies data on read instead of going to memory
   - Avoids writing to memory if block replaced
 - Shared - Up-to-date data, other caches may have a copy
 - Invalid - Data is not up-to-date, must fetch from memory to access
Modified
 - Only one cache has a valid copy of the block
 - The block is dirty
Owned
 - More than one cache has a valid copy of the block
 - The block is dirty and this cache is responsible for writing to memory
Exclusive
 - Only one cache has a valid copy of the block
 - The block is clean (up to date with memory)
Shared
 - More than one cache has a valid copy of the block
 - The cache may be either clean or dirty
 - May have multiple shared states for a block, but only one owner
 - Not responsible for writing to memory even if dirty
Invalid
 - Block is not up to date with changes and thus is unusable
Invalidate on write
 - When we write to a block, we choose to invalidate all other copies of the block, rather than updating the other copies

$$\begin{array}
\w{State}     & \w{Cache up to date?} & \w{Memory up to date?} & \w{Others have a copy?} & \w{Can respond to others' reads?} & \w{Can write to without changing states?} \\
\w{Modified}  & Y                     & N                      & N                       & Y                                 & Y                                         \\
\w{Owned}     & Y                     & M                      & M                       & Y?                                & N                                         \\
\w{Exclusive} & Y                     & Y                      & N                       & Y?                                & N                                         \\
\w{Shared}    & Y                     & M                      & M                       & N                                 & N                                         \\
\w{Invalid}   & N                     & M                      & M                       & N                                 & N                                         \\
\end{array}$$

## Virtual Memory

Page Table Maps Virtual Page Numbers (VPN) to Physical Page Numbers (PPN)

Direct-Mapped
 - caches $\Leftrightarrow$ page table
 - blocks $\Leftrightarrow$ pages
 - index $\Leftrightarrow$ VPN
 - data $\Leftrightarrow$ PPN

 - Virtual Address bits = $\log(\w{VA space}) - \w{offset}$
 - Physical Address bits = $\log(\w{PA space}) - \w{offset}$
 - PTE bits = 1(V) + 2(R/W) + 1(D) + (PPN)
 - PT size = $2^\w{VPN} * \w{PTE}$ bits
 - How many PTEs can be valid at a time? $2^\w{PPN}$

 - TLB - cache for page table
 - Fully associative, LRU Replacement
 - TLB Reach: memory addresses translatable
 - LRU bits = $\log_2$(entries)
 - TLBE bits = (VPN) + (valid, dirty, read, write) + (PPN) + (LRU)
 - TLB size = entries * TLBE bits
 - TLB reach = entries * page size

CPU $\xrightarrow{V. Addr}$ translation unit $\xrightarrow{P. Addr}$ memory unit

 - Each process has its own Page Table
 - Page Table Base Register points to the Page Table of the process currently running
 - TLB invalidated when swapping processes

single vs multilevel page table

## Parallelism

 - Fork: Master thread creates a team of parallel threads 
 - Join: When the team threads complete the statements in the parallel region construct, they synchronize and terminate, leaving only the master thread
 - Amdahl’s Law: $\frac{1}{1-F + \frac{F}{S}}$ ($F$ = Fraction sped up, $S$ = Amt of Speedup)

atomic operations
 - `amoswap` - test and set
 - lock is stored in memory at location `R[rs1]`, 1 if locked
 - we set it to 1, then check its value
 - if we see the value was previously already 1, we retry
 - if the value was previously 0, we set it to 1, and continue with critical section
 - acquire

```assembly
start: mv t0, 1
amoswap.w.aq t1, t0, a0 # t1 = M[R[a0]], M[R[a0]] = t0
bne t1, x0, start # keep retrying
amoswap.w.rl x0, x0, a0 # release when done
```

 - `lr`: we read value at a place in memory and try to reserve the address, no threads can write to location marked reserved
 - `sc`: if we got the reservation, store a word at the address; set `rd` to 0 if successful else 1, used to set lock
 - otherwise, start from beginning

## Warehouse Computing

 - PUE = Total Power / IT Power
 - Mean Time To Failure (MTTF) (how long you can expect a failure starting from a functional state) = 1 / Fails per period
 - Mean Time To Repair/Recover (MTTR) (duration of recovery after a failure)
 - Mean time between failures (MTBF): MTTF + MTTR
 - Availability = MTTF / (MTTF + MTTR)
 - Annualized Failure Rate: Avg. # of failures per year

## Recovery

 - RAID 1 - complete duplication, allows for concurrent reads
 - RAID 3 - parity bit
 - RAID 4 - transfer blocks rather than bits
 - RAID 5

Hamming ECC: have a parity bit at every $2^n$ locations (indexed by 1) coverying every other $2^n$

to detect error, add indexes of failing parity bits

## I/O

Memory Mapped I/O
 - Certain ranges of memory don’t actually address main memory, instead they point to I/O devices

Polling
 - Keep checking if data is available, if so do read
 - bad if time spent polling bogs down CPU performance

Interrupts
 - Register a function to be called when data is available
 - high overhead

Disk Access Time = Seek Time + Rotation Time + Transfer Time + Controller Overhead
 - Seek Time: time to position the head assembly at the proper cylinder
 - Rotation Time: time for the disk to rotate to the point where the first sectors of the block to access reach the head
 - Transfer Time: time taken by the sectors of the block and any gaps between them to rotate past the head

</textarea>