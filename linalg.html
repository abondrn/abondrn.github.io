<!DOCTYPE html>
<html lang="en">
<title>Linear Algebra</title>
<link rel="shortcut icon" href="ab.png" >
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128390657-1"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-128390657-1', {
  'page_path': location.pathname,
  'link_attribution': true,
});

gtag('event', 'page_view', { 'send_to': 'UA-128390657-1'});

(adsbygoogle = window.adsbygoogle || []).push({
google_ad_client: "ca-pub-6187072965455073",
enable_page_level_ads: true
});

window.addEventListener('load', function()
{
   if(window.ga && ga.create) console.log('Google Analytics is loaded');
   else console.log('Google Analytics is not loaded');    

   if(window.google_tag_manager) console.log('Google Tag Manager is loaded');
   else console.log('Google Tag Manager is not loaded');
}, false);

window.texme = { style: 'plain' }
</script>
<script src="https://cdn.jsdelivr.net/npm/texme@0.5.0"></script>
<style>
a {
  text-decoration: inherit;
}
</style>
<img src="https://ga-beacon.appspot.com/UA-128390657-2/gh-pages/linalg?pixel" />
<textarea>

# Linear Algebra

$$
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\T}{^\top}

\newcommand{\op}[1]{\operatorname{#1}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\times}
% arrow vectors
\newcommand{\bv}{\vec{b}}
\newcommand{\vv}{\vec{v}}
\newcommand{\xv}{\vec{x}}
\newcommand{\zv}{\vec{0}}
% operators
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\rows}{rows}
\DeclareMathOperator{\cols}{cols}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\Nul}{Nul}
$$

## Linear Operations

### Linear Map

 - **linearity** -- a property of a function $T$ that allows its result to be expressed as a combination of the function applied to the parts of the inputs summed and scaled individually; also known as the *superposition principle*, it is based on 2 properties:
   - **homogeniety** -- $T(cx) = c T(x)$
   - **additivity** -- $T(x + y) = T(x) + T(y)$

 - *linear combination* -- the weighted sum of vectors $c_1\vv_1 + \dots + c_k\vv_k$ that produces a vector, where $c_1,\dots,c_k$ are scalar coefficients

### The Column-view of Matrices: Vectors

 - *vector* -- can be represented as an sequence of numbers, where addition, scaling, and equality checks are all performed entry-wise

 - *zero vector $\zv$* -- a vector where all entries are 0; the additive identity

 - $r \x c$ matrix $A$ -- can be represented as a rectangular array of numbers arranged in $r$ rows and $c$ columns
 - any linear map $T$ defined on vectors can be performed by multiplication by a standard matrix A; $T(\xv) = A \cdot \xv$
   - uniquely defined by $([T(e_1) \dots T(e_c)]$, where $e_i = \col_i(I_c)$

   - $A_{rc}$ is the entry at the $r$th row and $c$th column; $A$ can then be written

$$\begin{bmatrix}
A_{11} & \dots  & A_{1c} \\
\vdots & \ddots & \vdots \\
A_{r1} & \dots  & A_{rc}
\end{bmatrix}$$

   - like vectors, addition, scaling, and equality are defined entry-by-entry

   - by convention, unless otherwise noted, a vector acts like a matrix with 1 column; if this is the case, it is said to be a **column vector**, like so:

$$\vv = \begin{bmatrix}
v_1    \\
\vdots \\
v_n
\end{bmatrix}$$

 - *matrix-vector product* $A\xv$ -- linear combination of the columns of $A$ specified by the corresponding entries of $\xv$; thus to perform, $A$ must have as many columns as $\xv$ rows

 - *identity matrix* $I_n$ -- an $n \x n$ matrix with 1 on the *main diagonal* (entries $A_{rc}$ where $r=c$) and 0 elsewhere
   - $I_n \xv = \xv$ for appropriate $n$

## Vector Spaces

 - $\R^n$ is the set of all $n$-vectors

 - $\Span\{v_1,\dots,v_n\}$ -- the set of all vectors that can be expressed as a linear combination of $v_1,\dots,v_n$

 - possible properties of vector sets
   - **spanning set for $H$** -- $V$ spans $H$ when $\Span V = H$
   - **linear independence** -- no vector in the set can be created as a linear combination of the others
     - $c_1v_1 + \dots + c_1v_n = 0$ except for $c_1,\dots,c_n = 0$
     - *linearly dependent* when linear combination with not-all-zero coefficients gives $\xv$
       - inevitable when there are more vectors than entries per vector
       - indicates that there is a redundancy in a set of vectors, as one vector is in the span of the others, so removing it doesn't change their span
   - **basis $\cal B$ for $H$** -- spanning and linearly independent
     - $v = c_1v_1 + \dots + c_nv_n$ for every $v \in H$ and unique $c_1,\dots,c_n$

 - **linear subspace** -- subset of a vector space containing $\zv$ closed under scaling, addition, and thus any linear combination
   - ${\zv}$, any span of $n$-vectors, and $\R^n$ are all the subspaces of $\R^n$

### Solution Sets

 - consider the matrix equation $A\xv = \bv$ for unknown $\xv$

 - **homogenous system** -- $A\xv = \zv$
   - **trivial solution** -- $\xv = \zv$
   - **nontrivial solution** -- $\xv \ne 0$; occurs when there are free variables after row-reduction
   - $Nul(A)$ --

characterize solution sets to the matrix equation $A\xv = \bv$ for unknown $\xv$
intersect along infinite space (line, plane, etc)
when $Ax = b$ has a solution, the system is said to be consistent
by linearity, when $A\xv = \zv$ is consistent, any solutions can be found by $\xv = \xv_{part} + \xv_0$, where $\xv_{part}$ is any single solution to $A\xv_{part} = \bv$, displaced by a vector that satisfies $A\xv_0 = 0$

### The Row-view of Matrices: Linear Systems
 - **augmented matrix** $[A \vdots \bv]$
   - a **coefficient matrix** $A$ with the rightmost column being the appended *output vector* $\bv$
   - represents a linear system with $r$ simultaneous equations and $c$ unknowns

 - **elementary row operations**
   - ways to transform a matrix that are reversible (preserve the *row space*) and don't change the constraints between the rows (preserve the *solution set*)
     - multiplying a row by nonzero constant (dilation)
     - add or subtract a row to another
     - swapping rows (relabelling)

 - **pivot position** -- leading or leftmost nonzero entry in a row of REF

 - **row echelon form (REF)** -- the last nonzero entry in each column is either a pivot (in which case that column is a *pivot column* and corresponds to a *basic* variable to be solved for) or to the right of a pivot (in which case that column is *free*)

 - **reduced row echelon form (RREF)**
   - all pivots are normalized (= 1) and expressed as the only nonzero entry in their column
   - does not change the number of pivots as found in REF
   - each nonzero row represents an isolated variable eliminated from all other equations and expressed solely in terms of a constant and the negative of the sum of the free terms in its row

 - **free variable** -- not restricted to any particular value; determines the basic variables of the rows it's in

 - **Gaussian elimination**, or simply *row-reduction*
   - **forward elimination** obtains the REF of the matrix
   - **back substitution** is used to go from REF to RREF

### 4 Subspaces of a Matrix

 - solution set questions
   - **existence** -- does the solution set contain *at least* 1 element
   - **uniqueness** -- does the solution set contain *at most one* element

 - *domain*: space which T transforms, $\R^{\cols(A)}$
 - *codomain*: space T maps onto, $\R^{\rows(A)}$
 - *range*: set of all images $T(x)$

 - possible properties of linear tranformation $T : X \to Y$
   - $X$ is the domain and $Y$ is the codomain
   - *onto*: range encompasses codomain
     - $T(x)=b$ has a solution for any $b \in Y$
   - *one-to-one*: each $\xv$ in $\R^{\cols(A)}$ transforms to its own $\bv$ in $\R^{\rows(A)}$; when $A\xv = \zv$ has only the trivial solution
     - $\xv_1 \ne \xv_2 \implies A\xv_1 \ne A\xv_2$; alternatively, $A\xv_1 = A\xv_2 \implies \xv_1 = \xv_2$
     -  has a kernel of $\{0\}$, and $T(x)=0$ only for the trivial solution $x=0$

 - *bijective*: relabeling of $\R^n$

 - a **spanning set** for $H$ is a set of vectors $V$ such that span $H$, or $\Span H = V$

 - basis for a subspace - a linearly independent set of vectors that spans that subspace
   - minimu number required to completely span the space
   - standard basis for $\R^n$ -- $e_i = \col_i(I_n)$
   - $\cal B$ denotes the set ${b_1, \dots, b_n}$

 - every linearly independent set is a basis for its own span
a basis of $V$ can serve as a coordinate system for $V$
 - changing basis $\leftrightarrow$ multiplying by matrix of basis columns
 - change of bases -- $[x]_B = Bx$

 - $\dim V$ -- number of basis vectors for $V$
   - $\min \cols(A)$, where $\Col(A) = V$; minimum dimensions needed to uniquely identify any vector in $V$
   - $\dim {\zv} = 0$
   - $\dim \R^n = n$
   - $\rank A$ = $\dim \Col(A)$ = # pivot cols in REF $\le \min(\rows(A), \cols(A))$ = # linearly independent columns

$\dim \Nul(A)$ = # free vars

all subspaces are spans of vectors in the superspace

what gets mapped to $0$ by $T$, $\op{kernel}(T)$

pivot columns for reduced $A$ are standard basis vectors for column space (restricted to nonzero rows) for $A$

$\Col(A) = \Span\{\dots,\col_i(A),\dots\}$ = range of $T$

### Bases

standard

## Matrix Algebra

pre and post multiply

### Matrix Multiplication
 - matrix multiplication
   - viewing matrices as linear transformations, this is composition
   - dimensions of the new matrix is $\rows(A) \x \cols(B)$ if $\cols(A) = \rows(B)$
     - inputs of first (of which there are $\cols(B)$) and outputs of second ($rows(A)$)
     - the output of the first, which is fed to the second, must correspond (are equal)
   - $\row_r(AB) = \row_r(A) B$, $\col_c(AB) = A \col_c(B)$, $(AB)_{rc} = \row_r(A) \cdot \col_c(B)$
   - distributive $A(B+C) = AB + BC$, $(B+C)A = BA + CA$
   - associative $A(BC) = (AB)C$
   - in general, NOT commutative ($AB \ne BA$)

<!--
method for keeping track of multiplication:
 / B
A AB
-->

### Transposes

transpose $A^T$ - flip $A$ along main diagonal
$(AB)^T$ = $B^TA^T$ - transpose of a product is the product of their transposes in reverse order
distributive $(A+B)^T$ = $A^T+B^T$

handedness
the transpose of a column vector is a row vector, and vice versa

### Inverses of Square Matrices

 - *singular*: has no inverse
 - elementary matrices
   - performs 1 elementary row operation on a matrix when left-multiplied
   - obtained by performing that operation on an identity matrix with the same number of rows
 - a matrix $A$ is (fully) invertible if there exists $A^{-1}$ such that $AA^{-1} = A^{-1}A = I_n$
 - algebraic properties:
   - idempotence -- ${A^{-1}}^{-1} = I$
   - composition -- $(AB)^{-1} = B^{-1}A^{-1}$ ($(AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = I$)
$(kA)^{-1} = k^{-1}A^{-1}$ ($kA = (kI)A$)
$(A^T)^{-1} = (A^{-1})^T$
invertible matrix theorem
method of computing inverse
 - construct $n \x 2n$ augmented matrix consisting of $A$ and $I$
 - row reduce this augmented matrix until the left side becomes the identity and the right side becomes $A^{-1}$
 - $[ A | I ]  \sim \dots \sim [ I | A^{-1} ]$

### Determinants

determinant denoted $\det A = \abs{A}$
cofactor - $C_{yx} A = (-1)^{x+y} \det(\text{$A$ w/o $y$th row and $x$th column})$ - sign alternating adjacently with top left being positive multiple (even)
cofactor expansion formula - sum of $A_{yx} C_{yx} A$ for a row/col
choose row/col with maximum 0s to minimize computation
if $A$ is (upper/lower) triangular, $\abs{A}$ = product of entries on the main diagonal
 - $\abs{kA} = k^n \abs{A}$
 - $\abs{A[i] += kA[j]} = \abs{A}$
 - $\abs{A[i] \leftrightarrow A[j]} = -\abs{A}$
 - $\abs{A[i] *= k} = 1/k \abs{A}$
 - $\abs{A^T} = \abs{A}$
 - $\abs{AB} = \abs{A}\abs{B}$
 - $\abs{A^{-1}} = \abs{A}^{-1}$

$A_x(b)$ - $A$, with column $x$ replaced by $b$

Cramer's rule - $x_i = \abs{A_i(b)}/\abs{A}$ for $i$ in $1..n$
 - $(\op{adj} A)_{yx} = C_{xy} A$
 - $A^{-1} = \op{adj} A/\abs{A}$
 - $\op{adj} A * A = \abs{A}I$

interpretation
 - area/volume/hypervolume formula for region defined by columns of A ('parallelogram') - $\abs{A}$
 - $(\text{area of region})\abs{A} = (\text{area of the transformed region})$

### Application: Flow Networks and Solving Circuits

### Application: Least-Squares Linear Regression

each row represents a measurement

least squares is orthogonal projection onto subspace spanned by columns of matrix
$A^T \vec{e} = \zv$

## The Diagonal Basis

### Application: Discrete-time Dynamical Systems
recurrence relation $\vec{s}[n+1] = A\vec{s}[n]$
 - a state transition matrix represents a directed graph, where edges represent the fraction of one state that moves to the next
 - example: pagerank model, population shift
 - entry $ij$ in matrix means fraction of quantity/material flowing from node $j$ (origin, source) entering node $i$ (destination) (rows $\leftrightarrow$ nodes, columns $\leftrightarrow$ edges)
 - columns sum to $1$ $\leftrightarrow$ total/system quantity conserved
 - row sums to $1$ $\leftrightarrow$ node quantity constant/invariant

##  Spectral Decomposition

$A$ is symmetric iff $A$ can be written as
$$A = UDU\T$$
where $D \in \R^{n \times n}$ is a diagonal matrix, and $U \in \R^{n \times n}$ is an orthogonal matrix

## Singular Value Decomposition

$X \in \R^{n \times d}$
can be written as
$X = U \Sigma V\T$,
where $U \in \R^{n \times n}$ is an orthogonal matrix ($U\T U = I_{n \times n}$),
$V \in \R^{d \times d}$ is an orthogonal matrix ($V\T V = I_{d \times d}$),
and $\Sigma \in \R^{n \times d}$ has nonnegative entries along its first $\min\{n, d\}$ diagonals, and zeros elsewhere.

### Compact SVD

when $n \ge d$, $\Sigma$ has the form
$$\Sigma = \begin{bmatrix}
\Sigma_{1:d} \\
0_{(n−d) \times d}
\end{bmatrix}
$$

Thus we save

$$X = U \Sigma V\T
= \begin{bmatrix}
U_{1:d} & U_{d+1:n}
\end{bmatrix} \begin{bmatrix}
\Sigma_{1:d}
\\
0_{(n−d) \times d}
\end{bmatrix}
V\T
= U_{1:d}\Sigma_{1:d}V\T
$$

$U_{d+1:n} \in \R^{ (n−d)}$

$\rank(X) = \rank(\Sigma)$, so the rank is the number of nonzero singular values

if we ordered the nonzero singular values $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r$,
then we have
$$X = U_{1:r}\Sigma_{1:r}V_{1:r}\T$$

</textarea>